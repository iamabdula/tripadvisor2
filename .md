# TripAdvisor Web Scraping Script

## Overview

This script automates the process of collecting information from the TripAdvisor website. It has two main parts:

1. **Crawler: [1_trip_advisor_crawler.py]**
   - **What it does:** The script starts by visiting TripAdvisor and searching for specific items.
   - **How it works:** It goes through each page, collects the links to individual items, and stores them in a CSV file.

2. **Scraper: [2_trip_advisor_scraper.py]**
   - **What it does:** After obtaining all the item links, it crawls thru each url and extract relevent data
   - **How it works:** This task is performed through multi threading, where we have main thread and worker thread. 
        **Main thread** Main thread manages worker threads
        **Worker Threads** Worker thread scrap the relevent data

## Other:
Different variables are created at the start of files like file names, workers count, Search query etc. 
# [locators.py] file
This file contains all the xpath annd locators for html page of trip advisor. These locators are usied in code files/.
# [live.png]
This file captures the screesnshot of last page
# [utils.py]
This file contains helper methods which are used across the project.
# [requirements.txt]
This file contains the installation requirements. Just create env and run python manage.py -r requirements.txt



## Part 1: Crawler 

### Workflow
1. Initialize a CSV file to store item links.
2. Open TripAdvisor, search for items, and go through pages.
3. For each page, collect item links and write them to the CSV.
4. Keep clicking "Next page" until all pages are processed.
5. Close the TripAdvisor window.

## Part 2: Scraper
1. Read item URLs from the CSV file generated in Part 1.
2. Divide the URLs among multiple worker threads.
3. Each worker thread:
4. Crawls and scrapes relevant data for the assigned URLs.
5. Updates the global records list with the extracted information.
6. Main thread manages the number of running threads and displays progress.
7. After all worker threads finish, the script updates the final CSV file with detailed information for each item.

## Note:
I have tried to implement the scraper using multi threading but due to certain constraints I have tested it with single worker.
I have optained the results from first and second part using a single worker. Since the site was detecting the bot, so I added
my chrome profile in settings to avoid that detection. And when I ran the code with multiple workers, it conflicted with my profile.
Other solutions to overcome the detection could have been Captcha solvers and Paid IPs but I did not have that. So I tried to scrap
as much data as possible.

## Results:

### Part1: 
Results from crawler (Part1) are stored in items_urls_Final.csv file
Total urls fetched: 13,386
Total time in mins: 33.6 min
## Records per minute ≈ [398.57]
## Records per second ≈ [6.63]

### Part2: 
Results from scraper (Part2) are stored in pages_Final.csv file
Total items fetched: 288
Total time in mins: 35.6 min
## Records per minute ≈ [7.5]
## Records per second ≈ [0.13]

# Abdullah Bin Masood